{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링\n",
    "\n",
    "##### 시간이 상당히 오래걸립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://news.daum.net/breakingnews/economic?regDate=20200617\n",
    "- 위 링크와 같이 , 경제 뉴스데이터를 활용\n",
    "- 링크 마지막부분에서 볼 수 있듯이 날짜또한 설정 가능\n",
    "- 1페이지당 15개의 뉴스가 있고 , 총 20페이지씩 크롤링\n",
    "- 즉 하루에 300개의 뉴스기사 제목만 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 일별 뉴스기사 url과 content 담기 위한 데이터프레임 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame(columns = ['date','url','content'])\n",
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 일별  하루당 300개의 뉴스기사 url 데이터  불러오기\n",
    "- 사용한 뉴스의 기간 : 2017/01/01 ~ 2020/06/17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url_1='https://media.daum.net/breakingnews/economic?regDate='\n",
    "#===============불러올 날짜 범위 설정\n",
    "\n",
    "import datetime\n",
    "\n",
    "#2017/01/01 ~ 2020/06/17 까지의 뉴스데이터 일별 url 로드\n",
    "\n",
    "start_date = datetime.date(2017, 1, 1)\n",
    "end_date   = datetime.date(2020, 6, 18) \n",
    "date_range = [(start_date + datetime.timedelta(n)).strftime('%Y%m%d') \\\n",
    "              for n in range(int ((end_date - start_date).days))]\n",
    "\n",
    "#==============페이지범위 1~20페이지까지\n",
    "url_2='&page='\n",
    "page= range(1,20)\n",
    "for i in tqdm(date_range):    \n",
    "    date_url = []\n",
    "    for j in page:\n",
    "        mainurl=url_1 +str(i)+ url_2 + str(j) \n",
    "        source_code_from_URL = urllib.request.urlopen(mainurl)\n",
    "        soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "        list_news = soup.find('ul', attrs={'class':'list_news2'})\n",
    "        list_news_li = list_news.find_all('li')\n",
    "        for item in list_news_li:\n",
    "            \n",
    "            try: #가끔 한두개의 기사에서 에러가 발생하였음 . \n",
    "                 #많지않은 데이터의 차이라서, 그냥 버리고자 예외문 사용\n",
    "                link_txt = item.find('a', attrs={'class':'link_txt'})\n",
    "                date_url.append(link_txt.get('href'))\n",
    "            except:\n",
    "                pass\n",
    "    news_df = news_df.append({'date' : i, 'url' : date_url}, ignore_index=True)\n",
    "news_df['date'] = pd.to_datetime(news_df['date'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 일별 url을 활용하여 뉴스기사 제목만 크롤링 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy\n",
    "from newspaper import Article\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "import time\n",
    "start_time = time.time()\n",
    "okt=Okt()\n",
    "index = 0\n",
    "\n",
    "for date in tqdm(news_df['date']):\n",
    "    day_content= ''    \n",
    "    for url in news_df.loc[news_df[news_df['date']==date].index[0],'url']:\n",
    "        try:\n",
    "            #기사데이터 불러오기\n",
    "            article=Article(url, language='ko')\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            #뉴스 기사 제목만 받아오는 과정, \n",
    "            news_title = article.title\n",
    "            \n",
    "            #정규표현식을 이용한 뉴스기사 정제\n",
    "            news_title= re.sub('[^가-힣0-9,]', repl= ' ',string =news_title)\n",
    "            x = ''.join(news_title)\n",
    "            day_content += x\n",
    "        except:\n",
    "            pass\n",
    "    news_df.loc[index, 'content'] = day_content\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### csv파일 생성    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv('뉴스데이터20200617.csv')\n",
    "news_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
